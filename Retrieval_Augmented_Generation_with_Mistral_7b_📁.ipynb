{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvULeFh3lsqc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'wikipedia-crypto-articles:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4264382%2F7344152%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240605%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240605T132852Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Daeba2962a379009d70c32b40633038a3fb9ab0f1c40337847e699c8ba8355f5e88d7ac38f524d970f5361360124e3fc3eb72d0b029db7bdea31088f6d2c6c9302f670955d5a8ad748e25c10328c49ac0cad28d8c4adf0f2b4b89df95d8300967e48991aae625d9adff8c7abbca742d3003fee15ea5d2e7f646daae146420d9d82ca08656f213d01d55a5207dbb47e2dd6c13b1334c5ec991d19d45b4b8f59c46ed3eeeeccda9e7cde2a7ecfa38cdb13ff06e4eede3658bae2b6dd188c7b0d1cdec6144e678f495e93b57f31902774e984419945fabda8c8541f36ec3e71a0298e245c8a5a1d3ead2478a61e1850b6caf5d3445619e93c0f68e3588be0d6265b1,mistral/pytorch/7b-instruct-v0.1-hf/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F3900%2F5112%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240605%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240605T132853Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0a9afb25b3c04467b999bd76d6aed3340940d14e154a31617bbc1a5adfd4a3164127120500abbeb7b043e9902323104c84ee48593891c38836416829065e7a0a234b1bc23cdd3783abfaec7d4290bbb73b794056d4e1d27795d4315cd299c9e8a3d3d47ebed37eac4c8e828ab4894b1ffabd56d71e11a3dc58eccf5436e7f2f15411eb874bec23854c766d803c08ee9612ca781934b49eb36d9cc822b4cb447ec9ada32faa285cfffe675caa0b6b5fd6f1347509c41e0eb983014866a986ac05b43c7619d351a05c3c18636abb287493122a77a7a1f71518fe08cccfa83fa525ece6ece0af5d61ed4fe18b0272eae5eb23866337b4b9b932e18272eb1b953433'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB9mmue_lsqg"
      },
      "source": [
        "<div style=\"font-family: 'Roboto', Arial, sans-serif; text-align: center;\">\n",
        "    <img src = \"https://i.imgur.com/sicKkOA.png\" width = 500, height= 500>\n",
        "    <div style=\"font-size: 42px; font-weight: bold; color: #353935; text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\">\n",
        "        <b>Retrieval Augmented Generation (RAG)<br> with Mistral 7b</b>\n",
        "    </div>\n",
        "    <hr style=\"border: none;\n",
        "               border-top: 1.25px solid #E0E0E0;\n",
        "               width: 75%;\n",
        "               margin-top: 20px;\n",
        "               margin-bottom: 20px;\n",
        "               margin-left: auto;\n",
        "               margin-right: auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39Ytpe0Blsql"
      },
      "source": [
        "<br><br><br>\n",
        "<div style=\"font-family: 'Roboto', Arial, sans-serif; text-align: left; font-size: 14px; letter-spacing: 1.5px; margin-top: 25px; margin-bottom: 25px;font-weight: bold;\">\n",
        "    Table of Contents\n",
        "</div>\n",
        "<br>\n",
        "\n",
        "- [Introduction](#intro)<br><br>\n",
        "- [Data Inspection](#eda)<br><br>\n",
        "- [Storing Data](#storing)<br><br>\n",
        "- [Loading Mistral 7b Model](#model)<br><br>\n",
        "- [Querying Model](#queries)<br><br>\n",
        "- [Conclusion](#conclusion)<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdU8XAzQlsqm"
      },
      "source": [
        "<div id = 'intro'\n",
        "     style=\"font-size: 42px; font-weight: bold; color: #353935; text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\">\n",
        "        <b>Introduction</b>\n",
        "    <hr style=\"border: none;\n",
        "               border-top: 1.25px solid #E0E0E0;\n",
        "               width: 100%;\n",
        "               margin-top: 20px;\n",
        "               margin-bottom: 20px;\n",
        "               margin-left: 0px;\n",
        "               margin-right: auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L23o-4Llsqm"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">Large language models are amazing tools that can help humans obtain answers to questions, summarize extensive texts, translate documents from one language to another, and help us code, among others.</p>\n",
        "\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">However, LLMs have one major issue: <b>hallucinations</b>. Hallucinations happen when an LLM spits out random facts from its training data, even if it may have no real connection to the user's prompt. Large language models have a hard time saying <i>\"I don't know\"</i> to questions they don't have an answer to.</p>\n",
        "\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\"><b>Retrieval-augmented generation (RAG)</b> is an AI framework that has two main objectives: Improve the quality of responses generated by connecting the model to an external source of knowledge and ensure that users have access to the model's sources so you can fact-check its answers for accuracy.</p>\n",
        "\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">With RAG, we can also ensure that the large language model has access to <b>proprietary data</b> by connecting it to custom sources of data from where it can retrieve information.</p>\n",
        "\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">The image below provides a clear understanding of how RAG works. First, a user makes a question to the LLM. Before reaching the model, the question reaches a <b>retriever</b>. This retriever will be responsible for looking up and retrieving relevant documents to answer the question from the <b>knowledge base</b>. The question, plus the relevant documents, will then be sent to the LLM, which will be able to generate a source-informed answer according to the sources from the documents it received.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsYrmzmLlsqm"
      },
      "source": [
        "<center>\n",
        "    <img src = \"https://assets-global.website-files.com/63f3993d10c2a062a4c9f13c/64593ba041a4ff8dfef73f30_1*LYApKuxzzmvFECqwYk61wg.png\">\n",
        "<p style = \"font-size: 16px;\n",
        "            font-family: 'Roboto', sans-serif;\n",
        "            text-align: center;\n",
        "            margin-top: 10px;\">Source: <a href = \"https://www.ml6.eu/blogpost/leveraging-llms-on-your-domain-specific-knowledge-base\">ml6.eu</a></p>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLKUXGvClsqn"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">In this notebook, we will implement a retrieval-augmented generation system for an LLM using the <a href=\"https://www.kaggle.com/datasets/lusfernandotorres/wikipedia-crypto-articles\">Wikipedia Crypto Articles</a>, a dataset that I uploaded a few days ago here on Kaggle.</p>\n",
        "\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">Before getting our hands dirty with code, let's install some relevant packages. These are:</p>\n",
        "\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">• <b><a href = \"https://www.trychroma.com/\">Chromadb</a>:</b> An open-source embedding database that allows us to plug LLMs to knowledge bases. It allows us to store and query embeddings and their metadata.</p>\n",
        "\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">• <b><a href = \"https://www.langchain.com/\">LangChain</a>:</b> A framework that allows us to develop several applications powered by LLMs.</p>\n",
        "\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">• <b><a href = \"https://pypi.org/project/sentence-transformers/\">Sentence Transformers</a>:</b> A framework that provides an easy method to compute dense vector representations for sentences, paragraphs, and images by leveraging pre-trained transformer models.</p>\n",
        "\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">• <b><a href = \"https://github.com/TimDettmers/bitsandbytes\">bitsandbytes</a>:</b> A library designed to optimize the training and deployment of large models through 4-bit quantization of the model's weights, reducing memory footprint and enhancing memory efficiency.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-01-08T17:40:08.088659Z",
          "iopub.status.busy": "2024-01-08T17:40:08.08819Z",
          "iopub.status.idle": "2024-01-08T17:40:25.671286Z",
          "shell.execute_reply": "2024-01-08T17:40:25.670102Z",
          "shell.execute_reply.started": "2024-01-08T17:40:08.088623Z"
        },
        "id": "FaHCin8Elsqo",
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Auto DataViz tool\n",
        "!pip install ydata-profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-01-08T17:40:25.673842Z",
          "iopub.status.busy": "2024-01-08T17:40:25.673557Z",
          "iopub.status.idle": "2024-01-08T17:41:00.934672Z",
          "shell.execute_reply": "2024-01-08T17:41:00.933562Z",
          "shell.execute_reply.started": "2024-01-08T17:40:25.673816Z"
        },
        "id": "zIPh4arWlsqo",
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Chromadb\n",
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-01-08T17:41:00.936351Z",
          "iopub.status.busy": "2024-01-08T17:41:00.936053Z",
          "iopub.status.idle": "2024-01-08T17:41:24.312394Z",
          "shell.execute_reply": "2024-01-08T17:41:24.311471Z",
          "shell.execute_reply.started": "2024-01-08T17:41:00.936325Z"
        },
        "id": "1z9-UCZ8lsq5",
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# LangChain\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-01-08T17:41:24.314036Z",
          "iopub.status.busy": "2024-01-08T17:41:24.313756Z",
          "iopub.status.idle": "2024-01-08T17:41:39.074569Z",
          "shell.execute_reply": "2024-01-08T17:41:39.073406Z",
          "shell.execute_reply.started": "2024-01-08T17:41:24.314005Z"
        },
        "id": "bttMDx1Dlsq6",
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Sentence Transformers\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-01-08T17:41:39.077649Z",
          "iopub.status.busy": "2024-01-08T17:41:39.077338Z",
          "iopub.status.idle": "2024-01-08T17:41:54.940661Z",
          "shell.execute_reply": "2024-01-08T17:41:54.93951Z",
          "shell.execute_reply.started": "2024-01-08T17:41:39.077622Z"
        },
        "id": "9pH2GFtulsq6",
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# bitsandbytes\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2024-01-08T17:41:54.942546Z",
          "iopub.status.busy": "2024-01-08T17:41:54.942143Z",
          "iopub.status.idle": "2024-01-08T17:42:04.895655Z",
          "shell.execute_reply": "2024-01-08T17:42:04.894656Z",
          "shell.execute_reply.started": "2024-01-08T17:41:54.942497Z"
        },
        "id": "_Tf-qe2mlsq7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Importing libs\n",
        "\n",
        "# Data Handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Auto EDA\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "\n",
        "# Torch and Transformers\n",
        "import torch\n",
        "from torch import bfloat16\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# LangChain\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import DataFrameLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Hiding warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2024-01-08T17:42:04.897584Z",
          "iopub.status.busy": "2024-01-08T17:42:04.896912Z",
          "iopub.status.idle": "2024-01-08T17:42:04.990998Z",
          "shell.execute_reply": "2024-01-08T17:42:04.99006Z",
          "shell.execute_reply.started": "2024-01-08T17:42:04.89755Z"
        },
        "id": "xO5RymRblsq7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory\n",
        "    total_memory_gb = total_memory / (1024**3) # Converting memory to Gb\n",
        "    print(\"GPU is available. \\nUsing GPU\")\n",
        "    print(\"\\nGPU Name:\", gpu_name)\n",
        "    print(f\"Total GPU Memory: {total_memory_gb:.2f} GB\")\n",
        "\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"GPU is not available. \\nUsing CPU\")\n",
        "    device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWZBoNZnlsq7"
      },
      "source": [
        "<div id = 'eda'\n",
        "     style=\"font-size: 42px; font-weight: bold; color: #353935; text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\">\n",
        "        <b>Data Inspection</b>\n",
        "    <hr style=\"border: none;\n",
        "               border-top: 1.25px solid #E0E0E0;\n",
        "               width: 100%;\n",
        "               margin-top: 20px;\n",
        "               margin-bottom: 20px;\n",
        "               margin-left: 0px;\n",
        "               margin-right: auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNZR5Ccflsq8"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">As I have previously mentioned, we are going to use the <a href=\"https://www.kaggle.com/datasets/lusfernandotorres/wikipedia-crypto-articles\">Wikipedia Crypto Articles</a> dataset as a knowledge source base for the model.</p>\n",
        "\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">We will use YData Profiling, an auto-visualization tool, to extract some info from the dataset with just a few lines of code.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T17:42:04.99226Z",
          "iopub.status.busy": "2024-01-08T17:42:04.991989Z",
          "iopub.status.idle": "2024-01-08T17:42:05.077542Z",
          "shell.execute_reply": "2024-01-08T17:42:05.0768Z",
          "shell.execute_reply.started": "2024-01-08T17:42:04.992238Z"
        },
        "id": "pN7RUZ1Ulsq9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Loading dataframe\n",
        "df = pd.read_csv('/kaggle/input/wikipedia-crypto-articles/Wikipedia Crypto Articles.csv')\n",
        "# Generating report\n",
        "report = ProfileReport(df, title = 'Wikipedia Crypto Articles')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2024-01-08T17:42:05.079074Z",
          "iopub.status.busy": "2024-01-08T17:42:05.078789Z",
          "iopub.status.idle": "2024-01-08T17:42:16.633662Z",
          "shell.execute_reply": "2024-01-08T17:42:16.632727Z",
          "shell.execute_reply.started": "2024-01-08T17:42:05.079044Z"
        },
        "id": "sGsC9I--lsq-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "report # Visualizing report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esUhk6Rolsq-"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">The dataset consists of two columns, <code>title</code> and <code>article</code>. We have nine entries with empty articles. We will remove these rows.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2024-01-08T17:42:16.635179Z",
          "iopub.status.busy": "2024-01-08T17:42:16.634867Z",
          "iopub.status.idle": "2024-01-08T17:42:16.642093Z",
          "shell.execute_reply": "2024-01-08T17:42:16.64114Z",
          "shell.execute_reply.started": "2024-01-08T17:42:16.635152Z"
        },
        "id": "eOR7yZWblsrA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Checking df length\n",
        "print('Dataframe Length:', len(df), 'rows')\n",
        "\n",
        "df = df.dropna() # Dropping empty entries\n",
        "\n",
        "# Checking df length after dropping empty articles\n",
        "print('Length After Dropping Empty Values:', len(df), 'rows')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV0ZY24vlsrB"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">Let's take a look at the content of the dataframe. I will print the title and the content of the text in <code>article</code> for the last entry.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2024-01-08T17:42:16.64372Z",
          "iopub.status.busy": "2024-01-08T17:42:16.643366Z",
          "iopub.status.idle": "2024-01-08T17:42:16.652174Z",
          "shell.execute_reply": "2024-01-08T17:42:16.651274Z",
          "shell.execute_reply.started": "2024-01-08T17:42:16.643688Z"
        },
        "id": "cp886UsElsrB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print('Title:', df.title.iloc[-1])\n",
        "print('\\n\\n\\n')\n",
        "print(df.article.iloc[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW_5sG1ElsrB"
      },
      "source": [
        "<div id = 'storing'\n",
        "     style=\"font-size: 42px; font-weight: bold; color: #353935; text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\">\n",
        "        <b>Storing Data</b>\n",
        "    <hr style=\"border: none;\n",
        "               border-top: 1.25px solid #E0E0E0;\n",
        "               width: 100%;\n",
        "               margin-top: 20px;\n",
        "               margin-bottom: 20px;\n",
        "               margin-left: 0px;\n",
        "               margin-right: auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUZIp43HlsrE"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">LangChain has tools called <i>Document Loaders</i> that allow us to load several types of data from a source as a <code>document</code>. A <code>document</code> contains text and associated metadata. We will use the <code>DataFrameLoader</code> class to load the data from a pandas DataFrame.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T17:42:16.653589Z",
          "iopub.status.busy": "2024-01-08T17:42:16.653264Z",
          "iopub.status.idle": "2024-01-08T17:42:16.661512Z",
          "shell.execute_reply": "2024-01-08T17:42:16.660586Z",
          "shell.execute_reply.started": "2024-01-08T17:42:16.65356Z"
        },
        "id": "_21K056MlsrE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Loading dataframe content into a document\n",
        "articles = DataFrameLoader(df,\n",
        "                           page_content_column = \"title\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T17:42:16.663033Z",
          "iopub.status.busy": "2024-01-08T17:42:16.662709Z",
          "iopub.status.idle": "2024-01-08T17:42:16.695078Z",
          "shell.execute_reply": "2024-01-08T17:42:16.694095Z",
          "shell.execute_reply.started": "2024-01-08T17:42:16.663Z"
        },
        "id": "ctFAKXdelsrF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Loading entire dataframe into document format\n",
        "document = articles.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om-T9ZDmlsrF"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">Before creating the embeddings from the document, we have to split it into smaller chunks. We do this for several reasons. </p>\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">First, embedding models may have a maximum token limit, and splitting the data ensures each chunk fits within these limits. </p>\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">Second, smaller chunks are more memory-efficient, which reduces computational costs. </p>\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">Third, embedding smaller and coherent segments of text may lead to higher accuracy and more meaningful representations. </p>\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">We will use LangChains's <code>RecursiveCharacterTextSplitter</code> to split our data.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T17:42:16.701739Z",
          "iopub.status.busy": "2024-01-08T17:42:16.700993Z",
          "iopub.status.idle": "2024-01-08T17:42:16.71507Z",
          "shell.execute_reply": "2024-01-08T17:42:16.71426Z",
          "shell.execute_reply.started": "2024-01-08T17:42:16.701705Z"
        },
        "id": "-v7CkPvslsrF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Splitting document into smaller chunks\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,\n",
        "                                chunk_overlap = 20)\n",
        "splitted_texts = splitter.split_documents(document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUhVRYljlsrF"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">We will use <code>HuggingFaceEmbeddings</code> to load a model from SentenceTransformers 🤗. More specifically, we will load the <code>sentence-transformers/all-MiniLM-L6-v2</code> model, which maps sentences and paragraphs to a 384-dimensional dense vector space. </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T17:42:16.716252Z",
          "iopub.status.busy": "2024-01-08T17:42:16.716017Z",
          "iopub.status.idle": "2024-01-08T17:42:22.612916Z",
          "shell.execute_reply": "2024-01-08T17:42:22.612129Z",
          "shell.execute_reply.started": "2024-01-08T17:42:16.716227Z"
        },
        "id": "0Sp16qAnlsrG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Loading model to create the embeddings\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNck_boGlsrH"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">With <code>Chroma</code>, we will create an indexed database of the embedded data. We use the <code>.from_documents( )</code> method from the Chroma class and input the chunks of text, <code>splitted_texts</code>, the embedding model, <code>embedding_model</code>, and we finally specify a directory where the indexed database will be stored.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T17:42:22.615459Z",
          "iopub.status.busy": "2024-01-08T17:42:22.614192Z",
          "iopub.status.idle": "2024-01-08T17:42:24.770115Z",
          "shell.execute_reply": "2024-01-08T17:42:24.769305Z",
          "shell.execute_reply.started": "2024-01-08T17:42:22.615398Z"
        },
        "id": "iqnonbP4lsrI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Creating and indexed database\n",
        "chroma_database = Chroma.from_documents(splitted_texts,\n",
        "                                      embedding_model,\n",
        "                                      persist_directory = 'chroma_db')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hEOBv0JlsrJ"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">You can see below that <code>chroma_database</code> is a vector store.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T17:42:24.771847Z",
          "iopub.status.busy": "2024-01-08T17:42:24.771262Z",
          "iopub.status.idle": "2024-01-08T17:42:24.777547Z",
          "shell.execute_reply": "2024-01-08T17:42:24.776543Z",
          "shell.execute_reply.started": "2024-01-08T17:42:24.77182Z"
        },
        "id": "hNLC6s0zlsrK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Visualizing the database\n",
        "chroma_database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TLNMwEblsrK"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">We use vector stores to store the embedded data. When we ask the model about something, the chain will embed this query and use it to retrieve the embedding vectors from the vector store according to the \"similarities\" between them and the embedded query. A vector store is simply responsible for storing the embedded data and performing a vector search for high-quality answers to our questions. </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryTXwZkblsrK"
      },
      "source": [
        "<center>\n",
        "    <img src = \"https://python.langchain.com/assets/images/vector_stores-125d1675d58cfb46ce9054c9019fea72.jpg\">\n",
        "<p style = \"font-size: 16px;\n",
        "            font-family: 'Roboto', sans-serif;\n",
        "            text-align: center;\n",
        "            margin-top: 10px;\">Source: <a href = \"https://python.langchain.com/docs/modules/data_connection/vectorstores/\">Vector stores on LangChain</a></p>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeePQAwwlsrK"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">Below, we define a retriever. Retrievers are responsible for retrieving the documents from the vector store based on a given query. They accept a string query as input and return a list of <code>document</code>s as output. </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T17:42:24.778896Z",
          "iopub.status.busy": "2024-01-08T17:42:24.778647Z",
          "iopub.status.idle": "2024-01-08T17:42:24.788507Z",
          "shell.execute_reply": "2024-01-08T17:42:24.787639Z",
          "shell.execute_reply.started": "2024-01-08T17:42:24.778873Z"
        },
        "id": "a4zQEwmjlsrK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Defining a retriever\n",
        "retriever = chroma_database.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2024-01-08T17:42:24.790225Z",
          "iopub.status.busy": "2024-01-08T17:42:24.789677Z",
          "iopub.status.idle": "2024-01-08T17:42:24.798364Z",
          "shell.execute_reply": "2024-01-08T17:42:24.797484Z",
          "shell.execute_reply.started": "2024-01-08T17:42:24.790192Z"
        },
        "id": "oFuqaqnvlsrL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Visualizing the retriever\n",
        "retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjSwKhPSlsrP"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">You can see above that the <code>retriever</code> object is an instance of the <code>VectorStoreRetriever</code>, and it is linked to the vector store <code>chroma_database</code>.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQV3Yb6OlsrQ"
      },
      "source": [
        "<div id = 'model'\n",
        "     style=\"font-size: 42px; font-weight: bold; color: #353935; text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\">\n",
        "        <b>Loading Mistral 7b Model</b>\n",
        "    <hr style=\"border: none;\n",
        "               border-top: 1.25px solid #E0E0E0;\n",
        "               width: 100%;\n",
        "               margin-top: 20px;\n",
        "               margin-bottom: 20px;\n",
        "               margin-left: 0px;\n",
        "               margin-right: auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD0AtdVxlsrQ"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">We can load the large language model with the <code>HuggingFacePipeline</code> class, which allows us to access over 120,000 open-source models publicly available on Huggingface.co. The model we will load is the <code>mistralai/Mistral-7B-v0.1</code> model for the <code>text-generation</code> task. </p>\n",
        "\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">The Mistral 7B is an open-source 7.3B parameter model that outperforms Meta's Llama 2 13B on all benchmarks. It is one of the most powerful open-source models available in January 2024.</p>\n",
        "\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">Since this is a large model, we are going to use the <code>bitsandbytes</code> library to create the <code>quantization_config</code> variable that is going to load the model in a 4-bit quantized format and enable double quantization. We are also setting the compute data type to bfloat16. These settings optimize the model's size and performance, avoiding memory usage issues.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T17:42:24.799553Z",
          "iopub.status.busy": "2024-01-08T17:42:24.799292Z",
          "iopub.status.idle": "2024-01-08T17:42:24.808061Z",
          "shell.execute_reply": "2024-01-08T17:42:24.807251Z",
          "shell.execute_reply.started": "2024-01-08T17:42:24.799524Z"
        },
        "id": "pnYo4M6WlsrR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Configuring BitsAndBytesConfig for loading model in an optimal way\n",
        "quantization_config = transformers.BitsAndBytesConfig(load_in_4bit = True,\n",
        "                                        bnb_4bit_quant_type = 'nf4',\n",
        "                                        bnb_4bit_use_double_quant = True,\n",
        "                                        bnb_4bit_compute_dtype = bfloat16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vs_x37YlsrS"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">We finally load the model in the <code>llm</code> variable. With the <code>model_kwargs</code> dictionary, we define a few behaviors for the model. For instance, <code>temperature</code> is a parameter that ranges from 0.0 to 1.0, and it defines how <i>\"creative\"</i> the model can be. Lower values make responses more predictable. <code>max_length</code> defines the maximum length of generated outputs, and <code>quantization_config</code> applies the previously defined quantization configuration to optimize the model.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T17:42:24.809641Z",
          "iopub.status.busy": "2024-01-08T17:42:24.80901Z",
          "iopub.status.idle": "2024-01-08T17:44:50.800436Z",
          "shell.execute_reply": "2024-01-08T17:44:50.799466Z",
          "shell.execute_reply.started": "2024-01-08T17:42:24.809612Z"
        },
        "id": "yYujGV8TlsrS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Loading Mistral 7b model\n",
        "llm = HuggingFacePipeline.from_model_id(model_id='/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1',\n",
        "                                       task = 'text-generation',\n",
        "                                       model_kwargs={'temperature': .3,\n",
        "                                                    'max_length': 1024,\n",
        "                                                    'quantization_config': quantization_config},\n",
        "                                       device_map = \"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IFNumVulsrS"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">In LangChain, we have modules called <b>Chains</b>,  a sequence of calls to an LLM. One of those chains is the <code>RetrievalQA</code>, which fetches relevant documents from an indexed database and then passes those documents into the LLM to generate a response. Let's define a Q&amp;A chain.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T17:44:50.80286Z",
          "iopub.status.busy": "2024-01-08T17:44:50.801735Z",
          "iopub.status.idle": "2024-01-08T17:44:50.808175Z",
          "shell.execute_reply": "2024-01-08T17:44:50.807297Z",
          "shell.execute_reply.started": "2024-01-08T17:44:50.802828Z"
        },
        "id": "SBSierb1lsrS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Defining a QnA chain\n",
        "QnA = RetrievalQA.from_chain_type(llm = llm,\n",
        "                                 chain_type = 'stuff',\n",
        "                                 retriever = retriever,\n",
        "                                 verbose = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZtFrPlvlsrT"
      },
      "source": [
        "<div id = 'queries'\n",
        "     style=\"font-size: 42px; font-weight: bold; color: #353935; text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\">\n",
        "        <b>Querying Model</b>\n",
        "    <hr style=\"border: none;\n",
        "               border-top: 1.25px solid #E0E0E0;\n",
        "               width: 100%;\n",
        "               margin-top: 20px;\n",
        "               margin-bottom: 20px;\n",
        "               margin-left: 0px;\n",
        "               margin-right: auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExUw5TrAlsrT"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">Below, I am going to define the <code>get_answers</code> function, which takes in the <code>QnA</code> chain we created above and a query, which is the question we make to the LLM.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T17:44:50.80995Z",
          "iopub.status.busy": "2024-01-08T17:44:50.809605Z",
          "iopub.status.idle": "2024-01-08T17:44:50.823379Z",
          "shell.execute_reply": "2024-01-08T17:44:50.822577Z",
          "shell.execute_reply.started": "2024-01-08T17:44:50.809917Z"
        },
        "id": "5Zc_eoZ7lsrT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Defining function to fetch documents according to a query\n",
        "def get_answers(QnA, query):\n",
        "    answer = QnA.run(query)\n",
        "    print(f\"\\033[1mQuery:\\033[0m {query}\\n\")\n",
        "    print(f\"\\033[1mAnswer:\\033[0m \", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI1sssZglsrT"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">We can now make questions to the model! Let's start with a few examples.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2024-01-08T17:44:50.824615Z",
          "iopub.status.busy": "2024-01-08T17:44:50.824359Z",
          "iopub.status.idle": "2024-01-08T17:44:56.783262Z",
          "shell.execute_reply": "2024-01-08T17:44:56.782349Z",
          "shell.execute_reply.started": "2024-01-08T17:44:50.824594Z"
        },
        "id": "R05NcJ1YlsrT",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "query = \"\"\"Who created the Bitcoin? When was it created?\"\"\"\n",
        "get_answers(QnA, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2024-01-08T17:44:56.784737Z",
          "iopub.status.busy": "2024-01-08T17:44:56.784445Z",
          "iopub.status.idle": "2024-01-08T17:45:00.134071Z",
          "shell.execute_reply": "2024-01-08T17:45:00.133178Z",
          "shell.execute_reply.started": "2024-01-08T17:44:56.784713Z"
        },
        "id": "J8VhlwvqlsrU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "query = \"\"\"What was the biggest scam in the history of cryptocurrencies?\"\"\"\n",
        "get_answers(QnA, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2024-01-08T17:45:00.13558Z",
          "iopub.status.busy": "2024-01-08T17:45:00.135256Z",
          "iopub.status.idle": "2024-01-08T17:45:01.675373Z",
          "shell.execute_reply": "2024-01-08T17:45:01.674425Z",
          "shell.execute_reply.started": "2024-01-08T17:45:00.135552Z"
        },
        "id": "dXccMqz6lsrU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "query = \"\"\"How much will one Bitcoin cost in 2030?\"\"\"\n",
        "get_answers(QnA, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2024-01-08T17:49:54.479001Z",
          "iopub.status.busy": "2024-01-08T17:49:54.478554Z",
          "iopub.status.idle": "2024-01-08T17:49:58.902802Z",
          "shell.execute_reply": "2024-01-08T17:49:58.90184Z",
          "shell.execute_reply.started": "2024-01-08T17:49:54.478963Z"
        },
        "id": "otUhMImUlsrU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "query = \"\"\"Cite the names of five relevant people in crypto?\"\"\"\n",
        "get_answers(QnA, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2024-01-08T17:45:05.913683Z",
          "iopub.status.busy": "2024-01-08T17:45:05.913353Z",
          "iopub.status.idle": "2024-01-08T17:45:10.083224Z",
          "shell.execute_reply": "2024-01-08T17:45:10.082232Z",
          "shell.execute_reply.started": "2024-01-08T17:45:05.913657Z"
        },
        "id": "PDjL_CfIlsrU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "query = \"\"\"What exchanges can I use to buy crypto?\"\"\"\n",
        "get_answers(QnA, query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2024-01-08T17:45:10.084646Z",
          "iopub.status.busy": "2024-01-08T17:45:10.084334Z",
          "iopub.status.idle": "2024-01-08T17:45:12.101624Z",
          "shell.execute_reply": "2024-01-08T17:45:12.100703Z",
          "shell.execute_reply.started": "2024-01-08T17:45:10.084621Z"
        },
        "id": "PgASLd1BlsrV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "query = \"\"\"Who conceived Ethereum?\"\"\"\n",
        "get_answers(QnA, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF4e0yWdlsrV"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">The model answers correctly when asked who created the Bitcoin and when it was created. I honestly don't know if the Squid Game scam was the biggest scam, but it was a scam that happened around 2021, and you can <a href =\"https://en.wikipedia.org/wiki/2021_Squid_Game_cryptocurrency_scam\">read about it on Wikipedia</a>. </p>\n",
        "\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">When asked about the price of one Bitcoin in 2030, the model answers with <b>\"I don't know\"</b>, which is good. We don't want the model to make guesses about information it doesn't have access to.</p>\n",
        "\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">When asked about the names of five relevant people in crypto,  the model repeats the name of <i>Andreas Antonopoulos</i> a few times, and even misspells his surname. There is definitely room for improvement here, but still, the model correctly named people who are related to cryptocurrencies.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdqBWp6slsrV"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">In the code below, we will use <code>query</code> to retrieve documents from our vector store and display the number of retrieved documents and the source of these documents, which is the title of the article as displayed on Wikipedia. I will also print the first 350 characters of the article text from the document's metadata so you can read a bit of it.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T17:45:12.103253Z",
          "iopub.status.busy": "2024-01-08T17:45:12.102888Z",
          "iopub.status.idle": "2024-01-08T17:45:12.140238Z",
          "shell.execute_reply": "2024-01-08T17:45:12.139303Z",
          "shell.execute_reply.started": "2024-01-08T17:45:12.103217Z"
        },
        "id": "Glh8o3rrlsrV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Obtaining the source and documents searched\n",
        "docs = chroma_database.similarity_search(query)\n",
        "print(f'Query: {query}')\n",
        "print(f'Retrieved documents: {len(docs)}')\n",
        "for doc in docs:\n",
        "    details = doc.to_json()['kwargs']\n",
        "    print(\"\\nSource (Article Title):\", details['page_content'])\n",
        "    print(\"\\nText\", details['metadata']['article'][:350] + \". . .\")\n",
        "    print('\\n\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-08T17:45:12.141652Z",
          "iopub.status.busy": "2024-01-08T17:45:12.141343Z",
          "iopub.status.idle": "2024-01-08T17:45:12.174846Z",
          "shell.execute_reply": "2024-01-08T17:45:12.173919Z",
          "shell.execute_reply.started": "2024-01-08T17:45:12.141626Z"
        },
        "id": "9tVhW3EjlsrV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Trying a different query\n",
        "query = \"\"\"What exchanges can I use to buy crypto?\"\"\"\n",
        "docs = chroma_database.similarity_search(query)\n",
        "print(f'Query: {query}')\n",
        "print(f'Retrieved documents: {len(docs)}')\n",
        "for doc in docs:\n",
        "    details = doc.to_json()['kwargs']\n",
        "    print(\"\\nSource (Article Title):\", details['page_content'])\n",
        "    print(\"\\nText\", details['metadata']['article'][:350] + \". . .\")\n",
        "    print('\\n\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDSXkfUnlsrW"
      },
      "source": [
        "<div id = 'conclusion'\n",
        "     style=\"font-size: 42px; font-weight: bold; color: #353935; text-shadow: 1px 2px 3px rgba(0,0,0,0.1);\">\n",
        "        <b>Conclusion</b>\n",
        "    <hr style=\"border: none;\n",
        "               border-top: 1.25px solid #E0E0E0;\n",
        "               width: 100%;\n",
        "               margin-top: 20px;\n",
        "               margin-bottom: 20px;\n",
        "               margin-left: 0px;\n",
        "               margin-right: auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB6Tg5BxlsrW"
      },
      "source": [
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">In this notebook, we have explored the <b>Retrieval-Augmented Generation (RAG)</b> as a process for improving the outputs of LLMs by giving them access to external data through an indexed vectorized database for retrieval of information.</p>\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">We have used the <a href =\"https://www.kaggle.com/models/mistral-ai/mistral/frameworks/PyTorch/variations/7b-instruct-v0.1-hf/versions/1\"><b>Mistral 7b model</b></a>, one of the most efficient open-source models of January 2024. For testing, we used the <a href =\"https://www.kaggle.com/datasets/lusfernandotorres/wikipedia-crypto-articles\"><b>Wikipedia Crypto Articles</b></a>, a dataset I have created by obtaining cryptocurrency-related articles from Wikipedia. </p>\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">I hope this notebook was convenient in helping you develop a deeper understanding of what RAG is. If you liked the content here, feel free to leave your upvote and feedback. I always love to read your comments and your insights into the subject.</p>\n",
        "<p style=\"font-family: 'Roboto', Arial, sans-serif; font-size: 20px; color: #353935;\">Thank you very much!</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2MfduZ4lsrW"
      },
      "source": [
        "<hr style=\"border: 0;\n",
        "           height: 1px;\n",
        "           border-top: 0.85px;\n",
        "           solid #b2b2b2\">\n",
        "           \n",
        "<div style=\"text-align: left;\n",
        "            color: #8d8d8d;\n",
        "            padding-left: 15px;\n",
        "            font-size: 14.25px;\">\n",
        "    Luis Fernando Torres, 2024<br><br>\n",
        "    Let's connect!🔗<br>\n",
        "    <a href=\"https://www.linkedin.com/in/luuisotorres/\">LinkedIn</a> • <a href=\"https://medium.com/@luuisotorres\">Medium</a> • <a href = \"https://huggingface.co/luisotorres\">Hugging Face</a><br><br>\n",
        "</div>\n",
        "<div style=\"text-align: center;\n",
        "            margin-top: 50px;\n",
        "            color: #8d8d8d;\n",
        "            padding-left: 15px;\n",
        "            font-size: 14.25px;\"><b>Like my content? Feel free to <a href=\"https://www.buymeacoffee.com/luuisotorres\">Buy Me a Coffee ☕</a></b>\n",
        "</div>\n",
        "<div style=\"text-align: center;\n",
        "            margin-top: 80px;\n",
        "            color: #8d8d8d;\n",
        "            padding-left: 15px;\n",
        "            font-size: 14.25px;\"><b>  <a href = \"https://luuisotorres.github.io/\">https://luuisotorres.github.io/</a> </b>\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Retrieval Augmented Generation with Mistral 7b 📁",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 4264382,
          "sourceId": 7344152,
          "sourceType": "datasetVersion"
        },
        {
          "modelInstanceId": 3900,
          "sourceId": 5112,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 30626,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
